{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9c522c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a18582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('SegmentAnalysis.pth')\n",
    "video = cv2.VideoCapture('../Videos/game_1.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "281e54e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define your model architecture\n",
    "class EventSegmentationLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        logits = self.classifier(out)\n",
    "        return logits\n",
    "\n",
    "# --- Step 1: Set your model hyperparameters ---\n",
    "input_dim = 1000     # replace with your actual input dimension\n",
    "hidden_dim = 128     # replace with your actual hidden dimension\n",
    "num_classes = 2     # replace with your actual number of classes\n",
    "\n",
    "# --- Step 2: Initialize the model ---\n",
    "model = EventSegmentationLSTM(input_dim, hidden_dim, num_classes)\n",
    "\n",
    "# --- Step 3: Load the state_dict ---\n",
    "state_dict_path = \"SegmentAnalysis.pth\"  # replace with your saved file path\n",
    "state_dict = torch.load(state_dict_path, map_location='cpu')  # use map_location if no GPU\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# --- Step 4: Set model to eval mode if needed ---\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b12846e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m     output = model(img_tensor)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Convert output to class\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m pred_idx = torch.argmax(\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m).item()  \u001b[38;5;66;03m# first batch, first timestep, all classes\u001b[39;00m\n\u001b[32m     38\u001b[39m pred_class = \u001b[33m\"\u001b[39m\u001b[33mClass A\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred_idx == \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mClass B\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Display prediction on frame\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model\n",
    "model = resnet18(pretrained=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Transform (same as training)\n",
    "transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Open video\n",
    "cap = cv2.VideoCapture(\"../Videos/rallies_01.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Apply transforms\n",
    "    img_tensor = transform(frame).unsqueeze(0).to(device)  # shape [1, 3, 224, 224]\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "\n",
    "    # Convert output to class\n",
    "    pred_idx = torch.argmax(output[0,0,:]).item()  # first batch, first timestep, all classes\n",
    "    pred_class = \"Class A\" if pred_idx == 0 else \"Class B\"\n",
    "\n",
    "    # Display prediction on frame\n",
    "    cv2.putText(frame, f\"Class: {pred_class}\", (30, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Video\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3d83d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
