{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a366f0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sreer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sreer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "023260d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(task=\"depth-estimation\", model=\"depth-anything/Depth-Anything-V2-Small-hf\")\n",
    "image = Image.open('../images/akash_001.jpg')\n",
    "depth = pipe(image)[\"depth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d13f0f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_depth_map(image):\n",
    "    depth = pipe(image)[\"depth\"]\n",
    "    depth_map = depth.resize(image.size)\n",
    "    return depth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02d9e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory_images(directory_path, save=False):\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            img_path = os.path.join(directory_path, filename)\n",
    "            img = Image.open(img_path)\n",
    "            depth_map = create_depth_map(img)\n",
    "            if save:\n",
    "                depth_map.save(os.path.join(directory_path, f\"depth_map_{filename}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a77d2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_directory_images('../images', save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257a8f53",
   "metadata": {},
   "source": [
    "## Depth Analysis Based Table Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abacbbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2  \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f04a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_frame(frame, yolo_results):\n",
    "    \"\"\"Draw YOLO keypoints on frame\"\"\"\n",
    "    annotated = frame.copy()\n",
    "    for r in yolo_results:\n",
    "        # check if keypoints exist and have at least 4 keypoints\n",
    "        if hasattr(r, 'keypoints') and r.keypoints is not None:\n",
    "            keypoints = r.keypoints.cpu().numpy()[0]  # first detection, shape (num_keypoints, 3)\n",
    "            if keypoints.shape[0] >= 4:\n",
    "                for kp in keypoints:\n",
    "                    x, y = kp[:2]  # take only x, y, ignore confidence\n",
    "                    cv2.circle(annotated, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "            else:\n",
    "            # fallback to bounding box if not enough keypoints\n",
    "                if hasattr(r, 'boxes') and r.boxes is not None:\n",
    "                    box = r.boxes.xyxy.cpu().numpy()[0]  # [x1, y1, x2, y2]\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        else:\n",
    "            # fallback to bounding box if keypoints attribute missing or None\n",
    "            if hasattr(r, 'boxes') and r.boxes is not None:\n",
    "                box = r.boxes.xyxy.cpu().numpy()[0]  # [x1, y1, x2, y2]\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "    return annotated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56bfa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 table, 144.0ms\n",
      "Speed: 3.4ms preprocess, 144.0ms inference, 201.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 table, 20.4ms\n",
      "Speed: 2.0ms preprocess, 20.4ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 16.4ms\n",
      "Speed: 2.2ms preprocess, 16.4ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m yolo_results = yolo_model(frame)  \u001b[38;5;66;03m# frame still BGR\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# --- Annotate frame with YOLO keypoints ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m annotated_frame = \u001b[43mannotate_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myolo_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Combine annotated frame and depth map side by side\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# combined = np.hstack((annotated_frame, depth_color))\u001b[39;00m\n\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Show frame\u001b[39;00m\n\u001b[32m     36\u001b[39m cv2.imshow(\u001b[33m\"\u001b[39m\u001b[33mYOLO + Depth\u001b[39m\u001b[33m\"\u001b[39m, annotated_frame)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mannotate_frame\u001b[39m\u001b[34m(frame, yolo_results)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m yolo_results:\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# check if keypoints exist and have at least 4 keypoints\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(r, \u001b[33m'\u001b[39m\u001b[33mkeypoints\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m r.keypoints \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m         keypoints = \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeypoints\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# first detection, shape (num_keypoints, 3)\u001b[39;00m\n\u001b[32m      8\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m keypoints.shape[\u001b[32m0\u001b[39m] >= \u001b[32m4\u001b[39m:\n\u001b[32m      9\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m kp \u001b[38;5;129;01min\u001b[39;00m keypoints:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sreer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ultralytics\\engine\\results.py:187\u001b[39m, in \u001b[36mBaseTensor.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m    171\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[33;03m    Return a new BaseTensor instance containing the specified indexed elements of the data tensor.\u001b[39;00m\n\u001b[32m    173\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    185\u001b[39m \u001b[33;03m        tensor([1, 2, 3])\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;28mself\u001b[39m.orig_shape)\n",
      "\u001b[31mIndexError\u001b[39m: index 0 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "yolo_model = YOLO(\"../yolov8-pose/runs/pose/train5/weights/best.pt\")\n",
    "pipe = pipeline(task=\"depth-estimation\", model=\"depth-anything/Depth-Anything-V2-Small-hf\")\n",
    "cap = cv2.VideoCapture(\"../Videos/rallies_01.mp4\")   # Replace with \"video.mp4\" for a file\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # --- Depth estimation ---\n",
    "    # Convert OpenCV frame to PIL\n",
    "    pil_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # Run depth estimation\n",
    "    depth_result = pipe(pil_frame)  # returns a dict\n",
    "\n",
    "    # Get depth map (as PIL image)\n",
    "    depth_pil = depth_result[\"depth\"]\n",
    "    depth_map = np.array(depth_pil).astype(np.float32)\n",
    "\n",
    "    # Normalize and apply colormap for visualization\n",
    "    depth_norm = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    depth_color = cv2.applyColorMap(depth_norm.astype(np.uint8), cv2.COLORMAP_MAGMA)\n",
    "\n",
    "\n",
    "\n",
    "    # --- YOLO pose detection ---\n",
    "    yolo_results = yolo_model(frame)  # frame still BGR\n",
    "\n",
    "    # --- Annotate frame with YOLO keypoints ---\n",
    "    annotated_frame = annotate_frame(frame, yolo_results)\n",
    "\n",
    "    # Combine annotated frame and depth map side by side\n",
    "    # combined = np.hstack((annotated_frame, depth_color))\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"YOLO + Depth\", annotated_frame)\n",
    "    \n",
    "    # Save frame\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
